# DS-Containers/app/streamlit_app.py
import streamlit as st
import polars as pl
import plotly.express as px
import os
from datetime import datetime, timedelta
import subprocess
import time

st.set_page_config(page_title="Restaurant Expense EDA Dashboard", layout="wide")
st.title("ðŸ“Š Restaurant Expense Exploratory Data Analysis")
st.markdown("This dashboard visualizes expense data directly from CSV files in the `data/` directory.")

# --- Data Loading ---
@st.cache_data(ttl=600) # Cache data for 10 minutes
def load_data(data_dir="../data"):
    """
    Loads all CSV files from the specified directory into a single Polars DataFrame.
    Assumes CSVs are in the format generated by data_generator.py.
    """
    all_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]
    
    if not all_files:
        st.warning(f"No CSV files found in '{data_dir}'. Please generate some data first!")
        return pl.DataFrame() # Return empty Polars DataFrame

    df_list = []
    for f in all_files:
        try:
            # Use Polars to read CSV
            df = pl.read_csv(f)
            df_list.append(df)
        except Exception as e:
            st.error(f"Error loading {f}: {e}")
            continue
            
    if not df_list:
        return pl.DataFrame() # Return empty Polars DataFrame

    # Concatenate DataFrames using Polars
    combined_df = pl.concat(df_list, how="vertical")
    
    # Ensure 'date' column is datetime
    combined_df = combined_df.with_columns(
        pl.col('date').str.strptime(pl.Datetime, "%Y-%m-%d %H:%M:%S%.f").alias("date")
    )
    
    # Add time-based features for analysis using Polars expressions
    combined_df = combined_df.with_columns([
        pl.col('date').dt.weekday().alias('day_of_week_num'), # Numeric day of week (Monday=1, Sunday=7)
        pl.col('date').dt.month().alias('month'),
    ])
    
    return combined_df

# --- Function to run ML Experiment ---
def run_ml_experiment_script(data_file_path):
    """
    Runs the ml_experiment.py script as a subprocess.
    Assumes ml_experiment.py is in the same 'app' directory.
    Returns the MLflow run URL if successful, otherwise None.
    """
    script_path = os.path.join(os.path.dirname(__file__), "ml_experiment.py")
    mlflow_run_url = None
    
    try:
        result = subprocess.run(
            ["python", script_path, data_file_path],
            capture_output=True,
            text=True,
            check=True,
            encoding='utf-8', # Explicitly set encoding for subprocess output
            errors='replace' # Replace unencodable characters instead of failing
        )
        
        # Parse stdout to find the MLflow run URL
        for line in result.stdout.splitlines():
            if "View run" in line and "http" in line:
                match = re.search(r'(http://localhost:\d+/#/experiments/\S+)', line)
                if match:
                    mlflow_run_url = match.group(1)
                    break
        
        st.success("ML Experiment run successfully!")
        if mlflow_run_url:
            st.markdown(f"**New MLflow Run Created:** [View Run Details]({mlflow_run_url})")
        else:
            st.info("MLflow run URL not found in script output, but experiment ran.")
        
        with st.expander("Show full ML Experiment output"):
            st.code(result.stdout)
            if result.stderr:
                st.warning("ML Experiment produced stderr output:")
                st.code(result.stderr)
        
        return mlflow_run_url
    
    except subprocess.CalledProcessError as e:
        st.error(f"ML Experiment failed with exit code {e.returncode}. Error:")
        st.code(e.stderr.encode('utf-8', errors='replace').decode('utf-8'))
        return None
    except FileNotFoundError:
        st.error(f"Error: Python executable or ml_experiment.py not found. Check your PATH or script location.")
        return None
    except Exception as e:
        st.error(f"An unexpected error occurred: {e}")
        return None


# Load the data
df = load_data()

# --- NEW: Link to MLflow UI & Run Experiment Button ---
st.markdown("---")
st.markdown("For detailed ML experiment tracking, visit the [MLflow UI](http://localhost:5000).")

st.subheader("Upload Custom Data")
uploaded_file = st.file_uploader("Choose a CSV file", type="csv")

if uploaded_file is not None:
    # Get the path to the data directory relative to the Streamlit app
    data_dir_path = os.path.join(os.path.dirname(__file__), '..', 'data')
    os.makedirs(data_dir_path, exist_ok=True) # Ensure data directory exists

    # Save the uploaded file to the data directory
    file_path = os.path.join(data_dir_path, uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    st.success(f"Uploaded '{uploaded_file.name}' to data/ directory!")
    
    # Rerun the app to refresh the list of available CSVs and data
    st.experimental_rerun() # Use experimental_rerun for immediate refresh after upload


st.subheader("Trigger ML Experiment")
st.write("Click the button below to run an ML experiment with selected data, logging results to MLflow.")

# Get a list of available CSV files in the data/ directory
data_files_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
available_csvs = [f for f in os.listdir(data_files_dir) if f.endswith('.csv')]

if not available_csvs:
    st.warning("No CSV files found in the `data/` directory. Please generate or upload data first!")
    selected_csv = None
else:
    # Allow user to select a CSV file
    selected_csv_name = st.selectbox("Select CSV file to analyze:", available_csvs)
    selected_csv = os.path.join(data_files_dir, selected_csv_name)

if st.button("Run ML Experiment") and selected_csv:
    with st.spinner("Running ML experiment and logging to MLflow..."):
        mlflow_run_url_result = run_ml_experiment_script(selected_csv)
        if mlflow_run_url_result:
            st.success(f"ML Experiment completed. New run logged. [View Run in MLflow]({mlflow_run_url_result})")
        else:
            st.error("ML Experiment failed. Check logs above for details.")
    st.rerun() # Rerun the app to refresh the dashboard and show new MLflow data

st.markdown("---")
# --- END NEW ---

if df.is_empty():
    st.info("Dashboard cannot be displayed as no data was loaded.")
else:
    st.sidebar.header("Filter Data")

    all_products = ["All"] + df['product'].unique().sort().to_list()
    selected_product = st.sidebar.selectbox("Select Product Category", all_products)

    all_suppliers = ["All"] + df['supplier'].unique().sort().to_list()
    selected_supplier = st.sidebar.selectbox("Select Supplier", all_suppliers)

    min_date = df['date'].min().date()
    max_date = df['date'].max().date()
    date_range = st.sidebar.date_input("Select Date Range", value=(min_date, max_date), min_value=min_date, max_value=max_date)

    filtered_df = df.clone() if not df.is_empty() else pl.DataFrame() # Ensure filtered_df is initialized

    if selected_product != "All":
        filtered_df = filtered_df.filter(pl.col('product') == selected_product)
    
    if selected_supplier != "All":
        filtered_df = filtered_df.filter(pl.col('supplier') == selected_supplier)

    if len(date_range) == 2:
        start_date = datetime.combine(date_range[0], datetime.min.time())
        end_date = datetime.combine(date_range[1], datetime.max.time())
        filtered_df = filtered_df.filter(
            (pl.col('date') >= start_date) & (pl.col('date') <= end_date)
        )
    elif len(date_range) == 1:
        start_date = datetime.combine(date_range[0], datetime.min.time())
        filtered_df = filtered_df.filter(pl.col('date') >= start_date)

    if filtered_df.is_empty():
        st.warning("No data matches the selected filters. Please adjust your selections.")
        st.stop()

    st.subheader("Raw Data Preview (Polars DataFrame)")
    st.dataframe(filtered_df.head().to_pandas())

    st.subheader("Overall Expense Trends")
    df_sorted_for_trends = filtered_df.sort("date") 

    weekly_summary = df_sorted_for_trends.group_by_dynamic(
        index_column="date",
        every="1w",
        offset="1d"
    ).agg(
        pl.col("total_price").sum().alias("Total Expense")
    ).sort("date")
    
    weekly_summary = weekly_summary.rename({"date": "Week Start"})
    weekly_summary_pd = weekly_summary.to_pandas()
    
    fig_weekly_expense = px.line(
        weekly_summary_pd,
        x='Week Start',
        y='Total Expense',
        title='Total Expense Over Time (Filtered)',
        labels={'Week Start': 'Week Start Date', 'Total Expense': 'Total Expense ($)'}
    )
    st.plotly_chart(fig_weekly_expense, use_container_width=True)

    st.subheader("Expense Breakdown")

    col1, col2 = st.columns(2)

    with col1:
        st.write("#### Expense by Product Category")
        product_expense = filtered_df.group_by('product').agg(
            pl.col('total_price').sum().alias('total_price')
        ).sort('total_price', descending=True).to_pandas()
        
        fig_product = px.bar(
            product_expense,
            x='total_price',
            y='product',
            orientation='h',
            title='Total Expense by Product Category (Filtered)',
            labels={'total_price': 'Total Expense ($)', 'product': 'Product Category'}
        )
        st.plotly_chart(fig_product, use_container_width=True)

    with col2:
        st.write("#### Expense by Supplier")
        supplier_expense = filtered_df.group_by('supplier').agg(
            pl.col('total_price').sum().alias('total_price')
        ).sort('total_price', descending=True).to_pandas()
        
        fig_supplier = px.bar(
            supplier_expense,
            x='total_price',
            y='supplier',
            orientation='h',
            title='Total Expense by Supplier (Filtered)',
            labels={'total_price': 'Total Expense ($)', 'supplier': 'Supplier'}
        )
        st.plotly_chart(fig_supplier, use_container_width=True)

    st.write("#### Expense by Payment Method")
    payment_expense = filtered_df.group_by('payment_method').agg(
        pl.col('total_price').sum().alias('total_price')
    ).to_pandas()
    
    fig_payment = px.pie(
        payment_expense,
        values='total_price',
        names='payment_method',
        title='Expense Distribution by Payment Method (Filtered)'
    )
    st.plotly_chart(fig_payment, use_container_width=True)

    st.subheader("Detailed Expense Breakdown Table")
    st.dataframe(filtered_df.sort('date', descending=True).to_pandas())

    st.subheader("Summary Statistics")
    st.dataframe(filtered_df.describe().to_pandas())

    st.subheader("Average Daily Expense")
    daily_avg_sorted_df = filtered_df.sort("date")
    daily_avg = daily_avg_sorted_df.group_by_dynamic(
        index_column="date",
        every="1d"
    ).agg(
        pl.col("total_price").sum().alias("Daily Total")
    ).sort("date")

    daily_avg_pd = daily_avg.to_pandas()

    fig_daily_avg = px.bar(
        daily_avg_pd,
        x='date',
        y='Daily Total',
        title='Daily Total Expense (Filtered)'
    )
    st.plotly_chart(fig_daily_avg, use_container_width=True)
